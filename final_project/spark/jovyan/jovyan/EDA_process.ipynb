{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24540764-9c5b-4867-8e5e-b46d6b8da1e8",
   "metadata": {},
   "source": [
    "## 1. Ingest and Access Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccc55d3-d8dc-4268-abdd-04d554d77a66",
   "metadata": {},
   "source": [
    "using SQLalchemy to ingest our data from our postgres database into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da87bab8-9d75-49b2-8b88-99f462cba9d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "could not translate host name \"database-1.cxoaueie6ybk.us-east-2.rds.amazonaws.com\" to address: Name or service not known\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m engine \u001b[38;5;241m=\u001b[39m create_engine(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpostgresql+psycopg2://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdb_user\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdb_password\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdb_host\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdb_port\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdb_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Establish a connection to the database\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m conn \u001b[38;5;241m=\u001b[39m \u001b[43mpsycopg2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdb_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdb_port\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdbname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdb_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdb_user\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdb_password\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/psycopg2/__init__.py:122\u001b[0m, in \u001b[0;36mconnect\u001b[0;34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     kwasync[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124masync_\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124masync_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    121\u001b[0m dsn \u001b[38;5;241m=\u001b[39m _ext\u001b[38;5;241m.\u001b[39mmake_dsn(dsn, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 122\u001b[0m conn \u001b[38;5;241m=\u001b[39m \u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconnection_factory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwasync\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cursor_factory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     conn\u001b[38;5;241m.\u001b[39mcursor_factory \u001b[38;5;241m=\u001b[39m cursor_factory\n",
      "\u001b[0;31mOperationalError\u001b[0m: could not translate host name \"database-1.cxoaueie6ybk.us-east-2.rds.amazonaws.com\" to address: Name or service not known\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "from pyspark.sql import SparkSession\n",
    "# Database connection parameters\n",
    "db_host = 'database-1.cxoaueie6ybk.us-east-2.rds.amazonaws.com'\n",
    "db_port = 5432\n",
    "db_name = 'postgres'\n",
    "db_user = 'root'\n",
    "db_password = 'irODJyh3LQpE0V3OcE3o'\n",
    "\n",
    "# Create an engine to connect to the database\n",
    "engine = create_engine(f'postgresql+psycopg2://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}')\n",
    "\n",
    "# Establish a connection to the database\n",
    "conn = psycopg2.connect(\n",
    "    host=db_host,\n",
    "    port=db_port,\n",
    "    dbname=db_name,\n",
    "    user=db_user,\n",
    "    password=db_password\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13d7d0b-973c-4c1a-ab9a-1ba4bd653c06",
   "metadata": {},
   "source": [
    "### Accessing Data: Reading Data into Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3311b048-4fc1-478b-9f38-a21f27aa3bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL query to fetch data\n",
    "query = 'SELECT * FROM darwin'\n",
    "# Load data into a DataFrame\n",
    "df = pd.read_sql(query, con = conn)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bf19d9-0071-4c73-856b-85fd6283030e",
   "metadata": {},
   "source": [
    "#### Looking at an overview of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb1ba03-ed9f-44a3-bd1b-80fbc142f867",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ae7966-4038-4da8-b78b-df13579dd5fa",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning and Pre-processing\n",
    "\n",
    "#### Dropping unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567505f0-64cd-4a2b-95ca-ec14e8423aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['working_time_pass','estimated_time','source','actual_time','actual_time_class',\n",
    "              'source_instance','estimated_time_minutes','working_time_arrival','working_time_departure'], axis=1)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a802d4e-0c38-4593-9dea-e0e34b27a866",
   "metadata": {},
   "source": [
    "#### Checking for duplicate data in the data set and dropping thhem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2604100-ecd3-4d98-8f44-34ab62ad5a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc31b37-826d-4cb9-b8a5-c0a49d5f5de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927a76e6-e8e6-4443-82bb-00ac476f8e5a",
   "metadata": {},
   "source": [
    "#### Reading in a reference csv that helps to find the station names for every route, then drop all unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b6971e-67ca-4181-b088-dffb0330904b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"./Data/RailReferences.csv\")\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4e8b97-9586-4c37-9212-6a8b539562c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.drop([\"StationNameLang\",\"GridType\",\"AtcoCode\",\"Modification\",\"CreationDateTime\",\"ModificationDateTime\",\"RevisionNumber\",\"CrsCode\"],axis=1)\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c316388-13cd-4c41-b547-a0c14e938a32",
   "metadata": {},
   "source": [
    "#### Merging the rail reference dataframe and main dataframe to show station name, easting, and northing for every datapoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f4c167-1d63-484a-93ec-5d43561cd758",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(df, df2, left_on='train_platform', right_on='TiplocCode', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d04a556-3860-4115-8562-9e95a4b5651c",
   "metadata": {},
   "source": [
    "#### Dropping the tiplocCode column as it was redundant with the added information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b727699-9daa-4900-82a3-1142fe16be04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = merged_df.drop([\"TiplocCode\"],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2b2665-a261-4b52-bdc0-d3ed8e9b109e",
   "metadata": {},
   "source": [
    "#### Showing Dataframe after merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d66415-b0cc-48fb-b539-370fbd2f7a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de907450-8782-4efe-9e4a-1d58882fb86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b43933-6aef-4f59-8482-9cf1b6f8347c",
   "metadata": {},
   "source": [
    "#### Checking for nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a46e2fe-b890-45d3-9d6c-5e1cf090388b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97286cfd-490f-47d1-a069-569510d21017",
   "metadata": {},
   "source": [
    "#### Dropping the nulls in Easting and Northing before converting them to Latitude and Longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7ba567-e091-447b-9b23-b13d90e35beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['Easting', 'Northing'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aab038-a043-4fa6-8f86-7515888269f1",
   "metadata": {},
   "source": [
    "#### Installing bng_latlon package to convert northing and easting to latitude and longitude\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a55ce13-7684-4d67-b068-8055b2506dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bng_latlon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f72ea4-a265-4bd3-a67d-f01c878f0b63",
   "metadata": {},
   "source": [
    "#### Creating a user defined function to calculate those values and writing to new columns with the longitude and latitude values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebb90de-734e-48a1-8bad-051b9f60a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bng_latlon import OSGB36toWGS84\n",
    "def latlong(df):\n",
    "    lat_long = []\n",
    "    for i in range(len(df)):\n",
    "        lat_long.append( OSGB36toWGS84(df[i][0],df[i][1]) )\n",
    "    return lat_long\n",
    "df['Easting'] = df['Easting'].astype('int64')\n",
    "df['Northing'] = df['Northing'].astype('int64')\n",
    "df1 = df [['Easting','Northing']]\n",
    "values = (df1.values)\n",
    "lat_long = latlong(values)\n",
    "#Writing to dataframe\n",
    "lat = []\n",
    "long = []\n",
    "for i in range(len(lat_long)):\n",
    "    lat.append(lat_long[i][0])\n",
    "    long.append(lat_long[i][1])\n",
    "    \n",
    "df['Latitude'] = lat\n",
    "df['Longitude'] = long"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab7ba80-c160-4048-9eda-c5b27cb1882f",
   "metadata": {},
   "source": [
    "#### Creating a new dataframe that drops all null values and checking the amount of data left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74f0a4a-8ec8-4da2-bc4a-06faf58d56ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "na_df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4153584-7f43-4ca2-8ca1-e3c359062cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting an overview of the amount of rows dropped and if we should continue with the null dropped dataset\n",
    "na_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf151ef-9666-40f8-b135-1d1fa6fb6e9f",
   "metadata": {},
   "source": [
    "After dropping data we are left with ~100,000 datapoints which is ~25% of our total data. We think this is significant enough for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead09bdb-ecf9-465f-a1c4-56d936ce752b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = na_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d438f7bc-7ff0-44a7-9f95-c91656d14711",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67877605-df8d-486e-8d86-1a908ffc290c",
   "metadata": {},
   "source": [
    "#### Changing all dates or time columns to the correct data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e6857a-eddb-4bb4-a920-8c694ae3936f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import time\n",
    "#changing the columns with dates to datetime objects\n",
    "df['service_start_date'] = pd.to_datetime(df['service_start_date'])\n",
    "df['planned_time_arrival'] = pd.to_datetime(df['planned_time_arrival'], format='%H:%M:%S').dt.time\n",
    "df['planned_time_departure'] = pd.to_datetime(df['planned_time_departure'], format='%H:%M:%S').dt.time\n",
    "df['actual_arrival_time'] = pd.to_datetime(df['actual_arrival_time'], format='%H:%M:%S').dt.time\n",
    "df['actual_departure_time'] = pd.to_datetime(df['actual_departure_time'], format='%H:%M:%S').dt.time\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb55d83-b976-4407-9158-ddfb4e74f78e",
   "metadata": {},
   "source": [
    "#### Converting all time columns to datetime objects with the service_start_date as the date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a9480d-4038-4965-a7df-26c879c54fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "df['actual_departure_time'] = df.apply(lambda row: datetime.combine(row['service_start_date'].date(), row['actual_departure_time']), axis=1)\n",
    "df['planned_time_departure'] = df.apply(lambda row: datetime.combine(row['service_start_date'].date(), row['planned_time_departure']), axis=1)\n",
    "df['planned_time_arrival'] = df.apply(lambda row: datetime.combine(row['service_start_date'].date(), row['planned_time_arrival']), axis=1)\n",
    "df['actual_arrival_time'] = df.apply(lambda row: datetime.combine(row['service_start_date'].date(), row['actual_arrival_time']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bfa937-6f7b-46e9-a010-515c822f85e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a5c397-cace-4f48-ad76-26cd22d30f06",
   "metadata": {},
   "source": [
    "#### Creating user defined functions to check for cases of dates arrival times or actual departure times taking place the day after the service start day, and add a day to the column values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d19427-af2c-4a2a-895e-2073a9949d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "#Adding service_start_date to time objects to get a datetime for every time column\n",
    "def change_actual_departure(row):\n",
    "    #Check if times occur after the initial service start day and correcting their date to the next day\n",
    "    if row['actual_departure_time'] < row['planned_time_departure']: \n",
    "        return row['actual_departure_time'] + timedelta(days=1)\n",
    "    return row['actual_departure_time']\n",
    "\n",
    "def change_arrival(row):\n",
    "    #Check if times occur after the initial service start day and correcting their date to the next day\n",
    "    if row['actual_arrival_time'] < row['planned_time_arrival']: \n",
    "        return row['actual_arrival_time'] + timedelta(days=1)\n",
    "    return row['actual_arrival_time']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3accc41b-5373-4ea1-b965-12e05934eb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['actual_departure_time'] = df.apply(change_actual_departure, axis=1)\n",
    "df['actual_arrival_time'] = df.apply(change_arrival, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7735689c-81ef-4726-8145-742c3372ceb8",
   "metadata": {},
   "source": [
    "#### Creating a calculated time difference column to further calculate delayed departure and arrival minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f14e7c1-9c7e-4092-ac47-2fdf2f956ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import time, timedelta\n",
    "df['actual_vs_planned_arrival'] = df['actual_arrival_time'] - df['planned_time_arrival']\n",
    "\n",
    "df['actual_vs_planned_arrival']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859f2005-082e-4ec8-ad82-df26c179cf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import time, timedelta\n",
    "\n",
    "df['actual_vs_planned_departure'] = df['actual_departure_time'] - df['planned_time_departure']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fdead9-b832-472a-8722-833aa39aabfd",
   "metadata": {},
   "source": [
    "#### Creating delayed arrival and delayed departure minutes column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3050cb3-9504-45ab-ae00-53b550a67b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['delayed_arrival_min'] = (df['actual_vs_planned_arrival'].dt.total_seconds() / 60)\n",
    "\n",
    "df['delayed_departure_min'] = (df['actual_vs_planned_departure'].dt.total_seconds() / 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb7a566-c227-4e39-9e08-0707b91271cc",
   "metadata": {},
   "source": [
    "Function to calculate origin and destination for each trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a13997a-2aab-4543-8715-20743bf0a2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_origin_destination(group):\n",
    "    origin = group.iloc[0]['StationName']\n",
    "    destination = group.iloc[-1]['StationName']\n",
    "    return pd.Series({\n",
    "        'Origin': origin,\n",
    "        'Destination': destination\n",
    "    })\n",
    "# Group by route_id, unique_id, and service_start_date, then apply the function to calculate origin and destination\n",
    "origin_destination = df.groupby(['route_id', 'unique_id', 'service_start_date']).apply(calculate_origin_destination).reset_index()\n",
    "#Joining into original dataframe\n",
    "df = df.merge(origin_destination, on=['route_id', 'unique_id', 'service_start_date'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5c1a5a-0768-4fd1-b914-a3f2884f5a47",
   "metadata": {},
   "source": [
    "#### Getting an overview of the data before exporting to csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6def487-8480-487d-adc1-91bd0713b3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c5ac71-6f16-4938-b9a1-5340e9ecfc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0d6e47-fd86-4f14-a78a-3092acdbc3c4",
   "metadata": {},
   "source": [
    "Converting the dataframe into a csv file to be exported into other programs like PowerBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a842323-e4eb-4400-8cf4-567bba527bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./Data/cleanRailData.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d85e74-7f4e-4b5a-b208-b79c56854ba3",
   "metadata": {},
   "source": [
    "## 3. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccb51be-7908-4204-a8cc-3560580e43e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96df396b-e881-4829-be95-ef193f4cd403",
   "metadata": {},
   "source": [
    "#### Distribution of Categorical and Numerical Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238f584b-3b9f-4915-ac35-c6333dc67416",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numerical_cols = ['planned_time_arrival', 'planned_time_departure', 'actual_arrival_time', 'actual_departure_time', \n",
    "                  'delayed_arrival_min', 'delayed_departure_min']\n",
    "subplot_index = 1\n",
    "for col in numerical_cols:\n",
    "    plt.figure(figsize=(16, 20))\n",
    "    plt.subplot(len(numerical_cols),1,subplot_index)\n",
    "    sns.histplot(df[col], bins=7)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a6bb60-6881-4dd1-b6cf-6a8d11d48157",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['is_delayed_arrival', 'is_delayed_departure','service_start_date']\n",
    "#histograms for categorical variables\n",
    "subplot_index = 1\n",
    "for col in categorical_cols:\n",
    "    plt.figure(figsize=(16, 20))\n",
    "    plt.subplot(len(categorical_cols),1,subplot_index)\n",
    "    sns.countplot(x=col, data=df)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94441b9f-7b06-4be7-a552-395649c04672",
   "metadata": {},
   "source": [
    "#### What is the distribution of delays in minutes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1388637-f4cb-4169-b52a-cd6e86082387",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a df with only delayed arrival trains\n",
    "delayed_arrivals = df[df['is_delayed_arrival'] == True]\n",
    "# Plotting the histogram using matplotlib\n",
    "plt.hist(delayed_arrivals['delayed_arrival_min'], bins=5)\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Arrival Minutes Delayed Among Delayed Trips')\n",
    "plt.xlabel('Arrival Delay (Minutes)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8c177e-04ea-4460-9b31-be3ba0842585",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a df with only delayed arriva trains\n",
    "delayed_departures = df[df['is_delayed_departure'] == True]\n",
    "# Plotting the histogram using matplotlib\n",
    "plt.hist(delayed_arrivals['delayed_departure_min'], bins=5)\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Departure Minutes Delayed Among Delayed Trips')\n",
    "plt.xlabel('Departure Delay (Minutes)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b0341d-ec17-4b2a-ad90-e80c6da8e491",
   "metadata": {},
   "source": [
    "#### Correlation Analysis among numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d149b3f-effc-4792-88c2-38f45cecdd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[['train_length', 'delayed_arrival_min', 'delayed_departure_min', 'Latitude', 'Longitude']]\n",
    "corr_matrix = df1.corr()\n",
    "# Create a correlation heatmap\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='Blues', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Between Numerical Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea185a6-c2f0-46d6-a202-6a20cbba7664",
   "metadata": {},
   "source": [
    "We found an absolute correlation between arrival delay and departure delay and decided to make a scatter plot to visualize the impact of Departure Delay on Arrival time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad375a1-9d21-4225-956c-3191a7a766ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(df['delayed_departure_min'], df['delayed_arrival_min'], alpha=0.5)\n",
    "plt.xlabel('Departure Delay (Minutes)')\n",
    "plt.ylabel('Arrival Delay (Minutes)')\n",
    "plt.title('Impact of Departure Delay on Arrival Delay')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d1b87d-f020-4459-b58c-d1386e23f3a1",
   "metadata": {},
   "source": [
    "This strong correlation of 1.00 shows that departure delay is significantly impacting the arrival time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
